# Lecture 12

**2021-02-26 (F)**

Recording available here: 

Code playground here: https://github.com/MRU-CSIS-2503-202101-001/lecture-playground (do a git pull if you've already cloned this repo before; otherwise, you can clone it into your Eclipse workspace directory)

Anonymous question board here: https://onlinequestions.org/ (20210112)

**!!! Start Recording, JP !!!**

   
## checking in 

[poll link]()


## before we begin

- quack

---

## Review

- on Wednesday, we talked about **Maps** - ADT's that take a **key** and find that key's associated **value**.
- we saw that Maps typically have the following ops:
  - **put** (add)
  - **get** (well..get)
  - **remove**
  - **contains**
- typically, we're more interested in having fast get/contains (even if the puts/removes aren't that great)
- we saw that when we make a Map data structure, we will typically use a MapEntry, which holds a key/value pair
- we cobbled together part of an implementation using an array...that's as far as we'll go on that implementation, 'cause we can do better

## What's the prob with our array-backed Map?

What time complexities are involved here? 

Let's focus on the get/contains side of thing, since we typically want that to be as fast as possible.

Using a linked list suffers the same problem.

Sorting would improve things to O(log n) - we'll see why near the end of the course - which is pretty good. Still, we want O(1), dammit!

## Let's do better

What was one of the primary strengths of arrays?

**It allows random access - if we have an index, we can get what we want out of that index in O(1) time.**

## Here's a plan

Maybe we could store our MapEntry thingies in an array?

If we wanted to put MapEntry in our Map, we'd:

1. make a MapEntry out of the K/V pair, then
2. _somehow_ turn the key into an index, then
3. add the MapEntry into the array at that index!

And if we wanted to see a given K/V pair existed (i.e. do a contains operation), we'd:

1. _somehow_ turn the key into an index, then
2. look in our array at that index, and
3. if that slot was "empty", then we'd say the key wasn't in our map...but if the slot wasn't empty, we'd say the key **was** in our map

And if we wanted to get something out of our Map, we'd basically do the above steps, but at the last step, we'd return the value we found in the correct index

> _what order are all these operations?!?_

**we might have a winner here!**

### what wrinkles are there?

- hmmmm...we'll let that fester a while

### hash code and hash function

- we need to associate an integer with a key somehow. We'll call this integer a **hash code**.
- we'd want to make sure that if two objects are considered equal, then they should have the same hash code (why? ask yourself what would happen otherwise....)
- ideally, whatever we do to create a hash code should try to minimize the possibility of 2 non-equal objects being assigned the same hash code. We'll see why later.

    > _for example, if we're using Baby objects as keys, then if we have 2 babies (baby1 and baby2), ideally we wouldn't want to generate the same hash code for baby1 and baby2_

- the function that turns the key into a hash code is called a **hash function**

### but how do we turn a hash code into an index?!?

Small problem here: let's say the array we're using to store our MapEntry thingies has 10 slots...but our hash function generates hash codes that are bigger than 10?

Remember your favourite operator, **%**?

It's got a pretty cool property that's **very** useful.

### congrats - we've just discovered hash tables

We call this setup: an **array** plus a **hash function** a **hash table**. It's a super-useful data structure that implements our Map ADT.

## unfortunately...there's a couple of wrinkles here

1. choosing a good hash function can be tricky; fortunately, there are automated ways of making "pretty good" ones (hi, Eclipse)

    > _you just have to be aware of the concept of hash functions - you're not going to be tested on making one yourself - leave that to mathematicians and computer scientists with lots of time on their hands_
    
1. more importantly, **what happens when two keys are mapped to the same slot?!?**

    > this is known as a **collision**; dealing with collisions is called **collision resolution**


## let's do some collision resolution together

There are two main methods of collision resolution you need to know: the **open addressing** method and the **separate chaining** method. 

Instead of describing them, let's do a quick visual on how each one works, then work through some examples of seeing them in action - they'll make more sense that way.

### the visuals

1. open-addressed tables 
2. separate-chained tables (**buckets**! ... and linked lists again?!? why can't they just leave us alone?!?!)

### the examples

Let's work through these together: https://docs.google.com/document/d/1i5F5Lp4xtuVUyDYzhMitgK-KU5jYDoemEEVyVfcANsQ/edit?usp=sharing

We'll ignore the whole MapEntry thing for the first one - we just want to get a handle on how the different types of collision handling work.

You can get this example and solution, plus another, from here: https://drive.google.com/drive/folders/1usGutYaw5uxcYJXLc-0STymMm2qgfPvJ?usp=sharing

## clustering

See those "clumps" that are forming in our open addressed table? Those are called **clusters**. **Clustering** is a common term describing the formation of these clumps.

Clusters are bad, by the way. Why?

## the load factor

load factor = $\frac {n}{k}$

- **n** is the number of entries occupied in the hash table
- **k** is the number of buckets

The load factor is basically an indication of how "full" the hash table is.

### load factor in open-addressed tables

In an open-addressed table, the load factor shows how "full" the array's slots are: a factor of 0 means all the slots are empty; a factor of 1 means every single slot is full!

In an open-addressed hash table, as the load factor approaches 1 (i.e. the hash table nears becoming totally full), the performance starts to suck hard - you're constantly going to a place, finding it full, and then searching bucket by bucket. This takes time....like O(n) time!

It's a good idea to resize your backing array when your load factor reaches a certain threshold - in Java, the default is 0.75 - but what's the problem with resizing?...
 
### load factor in separate-chained tables

Notice that in a separate-chained table, we can easily have load factors > 1 - this is normal.

So unlike an open-addressed table, the load factor is more an indication of the _average number of items in each bucket_. Having a low load factor isn't great in this case (why?) ... but having a high one isn't great either (why?). The sweet spot? Around 1 - which is totally different than what we'd want in an open-addressed table. Isn't life grand?





## QUESTIONS?

Ask 'em!

---

**HERE ENDETH THE LESSON**

[EXIT POLL]()

---

## terms used

- [ ] **hash code**
- [ ] **hash function**
- [ ] **hash table**
- [ ] **bucket**
- [ ] **collision**
- [ ] **open addressing (collision resolution method)**
- [ ] **linear probing**
- [ ] **quadratic probing**
- [ ] **double hashing**
- [ ] **clustering**
- [ ] **(separate) chaining (collision resolution method)**
- [ ] **load factor** 

## your action items

- [ ] complete the coding topic test B prereqs
- [ ] complete the practice coding test B

## coming up

- [ ] Wednesday will be review for Friday's written test B
- [ ] Thursday is coding test B

**!!! Stop Recording, JP !!!**